"""Service layer for vulnerability scanning integration."""

import logging
from datetime import timedelta
from typing import Any, Dict, List, Optional

from django.conf import settings
from django.db import models, transaction
from django.utils import timezone

from .clients import DependencyTrackAPIError, DependencyTrackClient, OSVClient, VulnerabilityProviderError
from .models import (
    ComponentDependencyTrackMapping,
    DependencyTrackServer,
    TeamVulnerabilitySettings,
    VulnerabilityScanResult,
)

logger = logging.getLogger(__name__)


class StandardizedVulnerabilityData:
    """
    Standardized vulnerability data structure for consistent storage across providers.

    Both OSV and Dependency Track results are normalized to this format.
    """

    @staticmethod
    def normalize_vulnerability(vuln_data: Dict[str, Any], provider: str) -> Dict[str, Any]:
        """
        Normalize a single vulnerability to standard format.

        Args:
            vuln_data: Raw vulnerability data from provider
            provider: Provider name ('osv' or 'dependency_track')

        Returns:
            Standardized vulnerability dictionary
        """
        if provider == "osv":
            return {
                "id": vuln_data.get("id", "unknown"),
                "source": "OSV",
                "severity": vuln_data.get("severity", "unknown"),
                "title": vuln_data.get("summary", vuln_data.get("title", "")),
                "description": vuln_data.get("details", vuln_data.get("description", "")),
                "references": [
                    ref.get("url") if isinstance(ref, dict) else ref for ref in vuln_data.get("references", []) if ref
                ],
                "aliases": vuln_data.get("aliases", []),
                "cvss_score": StandardizedVulnerabilityData._extract_cvss_score(vuln_data),
                "component": {
                    "name": vuln_data.get("package_name") or vuln_data.get("component", {}).get("name", "unknown"),
                    "version": vuln_data.get("component", {}).get("version", "unknown"),
                    "ecosystem": vuln_data.get("component", {}).get("ecosystem", "unknown"),
                    "purl": vuln_data.get("component", {}).get("purl", ""),
                },
                "affected": vuln_data.get("affected", []),
                "published_at": vuln_data.get("published"),
                "modified_at": vuln_data.get("modified"),
                "provider_specific": {
                    "database_specific": vuln_data.get("database_specific", {}),
                    "ecosystem_specific": vuln_data.get("ecosystem_specific", {}),
                },
            }

        elif provider == "dependency_track":
            return {
                "id": vuln_data.get("vulnId", vuln_data.get("id", "unknown")),
                "source": vuln_data.get("source", "Dependency Track"),
                "severity": vuln_data.get("severity", "unknown").lower(),
                "title": vuln_data.get("title", ""),
                "description": vuln_data.get("description", ""),
                "references": vuln_data.get("references", []),
                "aliases": vuln_data.get("aliases", []),
                "cvss_score": (
                    vuln_data.get("cvssV3BaseScore") or vuln_data.get("cvssV2BaseScore") or vuln_data.get("cvss_score")
                ),
                "component": {
                    "name": vuln_data.get("component", {}).get("name", "unknown"),
                    "version": vuln_data.get("component", {}).get("version", "unknown"),
                    "ecosystem": "unknown",  # DT doesn't always provide this
                    "purl": vuln_data.get("component", {}).get("purl", ""),
                },
                "affected": vuln_data.get("affected", []),
                "published_at": vuln_data.get("published"),
                "modified_at": vuln_data.get("modified"),
                "provider_specific": {
                    "recommendation": vuln_data.get("recommendation", ""),
                    "cwes": vuln_data.get("cwes", []),
                    "cvssV3Vector": vuln_data.get("cvssV3Vector", ""),
                    "cvssV2Vector": vuln_data.get("cvssV2Vector", ""),
                },
            }

        else:
            raise ValueError(f"Unknown provider: {provider}")

    @staticmethod
    def _extract_cvss_score(vuln_data: Dict[str, Any]) -> Optional[float]:
        """Extract CVSS score from vulnerability data."""
        # Check for direct cvss_score field
        if "cvss_score" in vuln_data and isinstance(vuln_data["cvss_score"], (int, float)):
            return float(vuln_data["cvss_score"])

        # Handle case where severity is already processed (e.g., from OSV client)
        severity = vuln_data.get("severity")
        if isinstance(severity, str):
            # Already processed by client, no CVSS score available
            return None

        # Parse from severity array (raw OSV format)
        if isinstance(severity, list):
            for severity_item in severity:
                if isinstance(severity_item, dict) and severity_item.get("type") == "CVSS_V3":
                    try:
                        # Extract numeric score from CVSS string if available
                        score_str = severity_item.get("score", "")
                        if "/" in score_str:
                            # Format like "9.8/CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H"
                            score_part = score_str.split("/")[0]
                            return float(score_part)
                    except (ValueError, TypeError):
                        continue

        return None

    @staticmethod
    def normalize_scan_results(
        raw_results: Dict[str, Any], provider: str, scan_trigger: str = "upload"
    ) -> Dict[str, Any]:
        """
        Normalize complete scan results to standardized format.

        Args:
            raw_results: Raw results from provider
            provider: Provider name
            scan_trigger: How the scan was triggered

        Returns:
            Standardized scan results
        """
        # Standardize vulnerability count format
        vuln_count = raw_results.get("vulnerability_count", {})
        if not isinstance(vuln_count, dict):
            vuln_count = {"total": 0, "critical": 0, "high": 0, "medium": 0, "low": 0, "info": 0, "unknown": 0}

        # Ensure all required severity levels exist
        standardized_count = {
            "total": vuln_count.get("total", 0),
            "critical": vuln_count.get("critical", 0),
            "high": vuln_count.get("high", 0),
            "medium": vuln_count.get("medium", 0),
            "low": vuln_count.get("low", 0),
            "info": vuln_count.get("info", 0),
            "unknown": vuln_count.get("unknown", 0),
        }

        # Normalize findings format
        findings = []
        raw_findings = raw_results.get("findings", [])

        if provider == "osv":
            # OSV format: {"vulnerabilities": [...]} or direct list
            if isinstance(raw_findings, dict) and "vulnerabilities" in raw_findings:
                raw_vulns = raw_findings["vulnerabilities"]
            else:
                raw_vulns = raw_findings if isinstance(raw_findings, list) else []
        else:
            # DT format: direct list
            raw_vulns = raw_findings if isinstance(raw_findings, list) else []

        # Normalize each vulnerability
        for vuln in raw_vulns:
            try:
                # Skip if vuln is not a dictionary
                if not isinstance(vuln, dict):
                    logger.warning(f"Skipping non-dictionary vulnerability data: {type(vuln)}")
                    continue

                normalized_vuln = StandardizedVulnerabilityData.normalize_vulnerability(vuln, provider)
                findings.append(normalized_vuln)
            except Exception as e:
                vuln_id = vuln.get("id", "unknown") if isinstance(vuln, dict) else "unknown"
                logger.warning(f"Failed to normalize vulnerability {vuln_id}: {e}")
                continue

        # Extract provider-specific metadata
        scan_metadata = {}
        if provider == "osv":
            scan_metadata = {
                "raw_output": raw_results.get("raw_output", ""),
                "scan_summary": raw_results.get("scan_summary", ""),
                "parse_error": raw_results.get("parse_error", ""),
            }
        elif provider == "dependency_track":
            scan_metadata = raw_results.get("metrics", {})

        return {
            "provider": provider,
            "scan_trigger": scan_trigger,
            "vulnerability_count": standardized_count,
            "findings": findings,  # Direct list format for both providers
            "scan_metadata": scan_metadata,
            "scan_timestamp": raw_results.get("scan_timestamp", timezone.now().isoformat()),
            "cached": raw_results.get("cached", False),
        }


class VulnerabilityScanningService:
    """
    Service for managing vulnerability scanning across different providers.

    Handles provider selection, component mapping, and result caching with standardized storage.
    """

    def __init__(self):
        self.cache_ttl = getattr(settings, "VULNERABILITY_SCAN_CACHE_TTL", 3600)  # 1 hour default
        self.osv_client = OSVClient()

    def _get_environment_prefix(self) -> str:
        """
        Generate an environment prefix based on APP_BASE_URL for DT project naming.

        This helps differentiate projects between environments (dev, staging, prod)
        when using a shared Dependency Track instance.

        You can override the automatic detection by setting the DT_ENVIRONMENT_PREFIX
        environment variable.

        Examples:
        - https://app.sbomify.com -> "prod"
        - https://staging.sbomify.com -> "staging"
        - https://dev.sbomify.com -> "dev"
        - http://localhost:8000 -> "local"
        - DT_ENVIRONMENT_PREFIX="custom" -> "custom"

        Returns:
            Environment prefix string
        """
        import os
        from urllib.parse import urlparse

        from django.conf import settings

        try:
            # Check for explicit override first
            override_prefix = os.environ.get("DT_ENVIRONMENT_PREFIX", "").strip()
            if override_prefix:
                logger.info(f"Using explicit DT environment prefix: {override_prefix}")
                return override_prefix

            app_base_url = getattr(settings, "APP_BASE_URL", "")

            if not app_base_url:
                return "unknown"

            parsed = urlparse(app_base_url)
            hostname = parsed.hostname

            # Handle cases where hostname is None (invalid URLs)
            if not hostname:
                # Try to extract from netloc as fallback
                netloc_parts = parsed.netloc.split(":")
                if netloc_parts and netloc_parts[0]:
                    hostname = netloc_parts[0]
                else:
                    return "unknown"

            # Handle localhost/development
            if hostname in ["localhost", "127.0.0.1", "0.0.0.0"]:  # nosec B104
                return "local"

            # Extract environment from subdomain or domain (prioritize keyword matching)
            hostname_lower = hostname.lower()
            if "staging" in hostname_lower:
                return "staging"
            elif "dev" in hostname_lower or "development" in hostname_lower:
                return "dev"
            elif "test" in hostname_lower:
                return "test"
            elif "prod" in hostname_lower:
                return "prod"
            else:
                # For main domains like app.sbomify.com, sbomify.com - consider production
                if hostname_lower.endswith(".com") or hostname_lower.endswith(".org"):
                    # Check if it's a main domain or if we should use the subdomain
                    parts = hostname_lower.split(".")
                    if len(parts) >= 3:  # subdomain.domain.com
                        # Use the first part as environment if it's not a common app prefix
                        first_part = parts[0]
                        if first_part not in ["app", "www", "api"]:
                            return first_part
                    return "prod"
                else:
                    # Use the first part of the hostname as environment identifier
                    parts = hostname_lower.split(".")
                    return parts[0] if parts and parts[0] else "unknown"

        except Exception as e:
            logger.warning(f"Failed to determine environment prefix from APP_BASE_URL: {e}")
            return "unknown"

    def scan_sbom_for_vulnerabilities(self, sbom, sbom_data: bytes, scan_trigger: str = "upload") -> Dict[str, Any]:
        """
        Main entry point for vulnerability scanning.

        Determines the appropriate provider based on team settings and routes the scan.

        Args:
            sbom: SBOM instance
            sbom_data: SBOM file content as bytes
            scan_trigger: How the scan was triggered ("upload", "manual", "weekly", "api")

        Returns:
            Standardized scan results dictionary
        """
        try:
            team = sbom.component.team

            # Determine which provider to use
            if self.should_use_dependency_track(team, sbom_data):
                logger.info(f"Using Dependency Track for SBOM {sbom.id} (team: {team.key})")
                raw_results = self._scan_with_dependency_track(sbom, sbom_data)
                provider = "dependency_track"
            else:
                logger.info(f"Using OSV for SBOM {sbom.id} (team: {team.key})")
                raw_results = self._scan_with_osv(sbom, sbom_data)
                provider = "osv"

            # Standardize the results
            standardized_results = StandardizedVulnerabilityData.normalize_scan_results(
                raw_results, provider, scan_trigger
            )

            # Add SBOM metadata
            standardized_results.update({"sbom_id": str(sbom.id), "scan_timestamp": timezone.now().isoformat()})

            return standardized_results

        except Exception as e:
            logger.error(f"Vulnerability scan failed for SBOM {sbom.id}: {e}")

            # Preserve additional error data from VulnerabilityProviderError
            error_data = {"error": f"Vulnerability scan failed: {str(e)}", "vulnerability_count": {}, "findings": []}
            provider = "unknown"

            if isinstance(e, VulnerabilityProviderError) and hasattr(e, "response_data") and e.response_data:
                error_data.update(e.response_data)
                provider = e.response_data.get("provider", "unknown")

            return StandardizedVulnerabilityData.normalize_scan_results(error_data, provider, scan_trigger)

    def store_scan_results(
        self, sbom, standardized_results: Dict[str, Any], component_mapping: ComponentDependencyTrackMapping = None
    ) -> VulnerabilityScanResult:
        """
        Store standardized scan results in PostgreSQL.

        Args:
            sbom: SBOM instance
            standardized_results: Normalized scan results
            component_mapping: DT mapping if applicable

        Returns:
            Created VulnerabilityScanResult instance
        """
        # Create new result (keep all historical data)
        result = VulnerabilityScanResult.objects.create(
            sbom=sbom,
            provider=standardized_results["provider"],
            scan_trigger=standardized_results["scan_trigger"],
            component_mapping=component_mapping,
            vulnerability_count=standardized_results["vulnerability_count"],
            findings=standardized_results["findings"],
            scan_metadata=standardized_results["scan_metadata"],
        )

        logger.info(f"Stored standardized {standardized_results['provider']} results for SBOM {sbom.id}")
        return result

    def get_cached_results(self, sbom, provider: str = None) -> Optional[Dict[str, Any]]:
        """Get latest scan results from PostgreSQL (no expiration concept)."""
        try:
            result = VulnerabilityScanResult.get_latest_for_sbom(sbom, provider)

            if result:
                return {
                    "vulnerability_count": result.vulnerability_count,
                    "findings": result.findings,
                    "scan_metadata": result.scan_metadata,
                    "provider": result.provider,
                    "scan_trigger": result.scan_trigger,
                    "cached": False,  # No longer caching, just historical data
                    "sbom_id": str(sbom.id),
                    "scan_timestamp": result.created_at.isoformat(),
                }
        except Exception as e:
            logger.warning(f"Failed to get latest results for SBOM {sbom.id}: {e}")

        return None

    def should_use_dependency_track(self, team, sbom_data: bytes = None) -> bool:
        """
        Determine if Dependency Track should be used for this team and SBOM.

        CRITICAL: Dependency Track ONLY supports CycloneDX format, NOT SPDX!
        SPDX SBOMs must always use OSV regardless of team plan.

        Args:
            team: Team instance
            sbom_data: Optional SBOM content for format validation

        Returns:
            True if DT should be used, False for OSV
        """
        # Check if team has Business/Enterprise plan
        if not team.billing_plan or team.billing_plan not in ["business", "enterprise"]:
            return False

        # Get team settings
        team_settings = self.get_or_create_team_settings(team)

        # Check if team has explicitly chosen Dependency Track
        if team_settings.vulnerability_provider != "dependency_track":
            return False

        # CRITICAL: Check SBOM format - DT only supports CycloneDX
        if sbom_data:
            # Use proper format detection (content-based only)
            format_type = self.osv_client.detect_sbom_format(sbom_data)

            if format_type == "spdx":
                logger.warning(
                    f"Team {team.key} configured for Dependency Track but SBOM is SPDX format. "
                    f"Forcing OSV usage as DT only supports CycloneDX."
                )
                return False

            elif format_type == "cyclonedx":
                logger.debug(f"SBOM is CycloneDX format, compatible with Dependency Track for team {team.key}")
                return True

            else:  # unknown format
                logger.warning(
                    f"Team {team.key} configured for Dependency Track but SBOM format is unknown. "
                    f"Forcing OSV usage as DT only supports CycloneDX."
                )
                return False

        return True

    def get_or_create_team_settings(self, team) -> TeamVulnerabilitySettings:
        """Get or create team vulnerability settings."""
        settings, created = TeamVulnerabilitySettings.objects.get_or_create(
            team=team, defaults={"vulnerability_provider": "osv"}
        )

        if created:
            logger.info(f"Created default vulnerability settings for team {team.key}")

        return settings

    def _scan_with_osv(self, sbom, sbom_data: bytes) -> Dict[str, Any]:
        """Scan SBOM using OSV scanner."""
        try:
            # Check for recent results first (avoid duplicate scans)
            cached_result = self._get_cached_osv_results(sbom)
            if cached_result:
                return cached_result

            # Perform OSV scan
            scan_results = self.osv_client.scan_sbom(sbom_data, sbom.sbom_filename)

            # Add metadata
            scan_results.update(
                {
                    "sbom_id": str(sbom.id),
                    "scan_timestamp": timezone.now().isoformat(),
                    "cached": False,
                    "provider": "osv",
                }
            )

            # Store results in PostgreSQL
            self._cache_osv_results(sbom, scan_results)

            return scan_results

        except VulnerabilityProviderError as e:
            logger.error(f"OSV scan failed for SBOM {sbom.id}: {e}")
            error_result = {"error": str(e), "sbom_id": str(sbom.id), "provider": "osv"}

            # Include additional error data if available
            if hasattr(e, "response_data") and e.response_data:
                error_result.update(e.response_data)

            return error_result

    def _scan_with_dependency_track(self, sbom, sbom_data: bytes) -> Dict[str, Any]:
        """Scan SBOM using Dependency Track with intelligent upload/polling logic."""
        try:
            team = sbom.component.team

            # Select DT server
            dt_server = self.select_dependency_track_server(team)

            # Check if we have an existing mapping and recent upload
            mapping = self.get_or_create_component_mapping(sbom.component, dt_server)

            # Determine if we should upload or just poll for updates
            should_upload = self._should_upload_to_dt(sbom, mapping)

            if should_upload:
                logger.info(f"Uploading SBOM {sbom.id} to DT (new or outdated)")
                # Upload SBOM to DT (creates project if mapping is None)
                upload_result = self.upload_sbom_to_dependency_track(sbom, sbom_data, mapping)
                final_mapping = upload_result["mapping"]
                created_project = upload_result.get("created_project", False)

                if not final_mapping:
                    logger.error(f"No mapping available after DT upload for SBOM {sbom.id}")
                    return {
                        "error": "No project mapping available after upload",
                        "sbom_id": str(sbom.id),
                        "provider": "dependency_track",
                    }

                # Queue polling task for new upload
                from .tasks import poll_dependency_track_results_task

                poll_dependency_track_results_task.send(
                    str(sbom.id), str(final_mapping.id), max_attempts=6, initial_delay=30
                )

                # Return processing state
                return {
                    "status": "processing",
                    "provider": "dependency_track",
                    "sbom_id": str(sbom.id),
                    "dt_server": dt_server.name,
                    "dt_project_uuid": str(final_mapping.dt_project_uuid),
                    "created_project": created_project,
                    "scan_timestamp": timezone.now().isoformat(),
                    "message": "SBOM uploaded to Dependency Track. Processing vulnerability data...",
                    "action": "upload",
                    "vulnerability_count": {"critical": 0, "high": 0, "medium": 0, "low": 0, "info": 0, "total": 0},
                    "findings": [],
                    "scan_metadata": {"processing": True, "expected_completion": "2-5 minutes", "action": "upload"},
                }
            else:
                logger.info(f"Polling for updated vulnerability data for SBOM {sbom.id} (recent upload)")
                # Just poll for updated vulnerability data
                try:
                    scan_results = self.get_dependency_track_results(sbom, mapping, force_refresh=True)

                    # Update the last sync timestamp
                    mapping.last_metrics_sync = timezone.now()
                    mapping.save(update_fields=["last_metrics_sync"])

                    # Add metadata
                    scan_results.update(
                        {
                            "sbom_id": str(sbom.id),
                            "scan_timestamp": timezone.now().isoformat(),
                            "cached": False,
                            "dt_server": dt_server.name,
                            "dt_project_uuid": str(mapping.dt_project_uuid),
                            "action": "poll",
                        }
                    )

                    # Store updated results
                    self._cache_dt_results(sbom, mapping, scan_results)

                    logger.info(f"Successfully polled updated vulnerabilities for SBOM {sbom.id}")
                    return scan_results

                except Exception as poll_error:
                    logger.warning(f"Failed to poll DT updates for SBOM {sbom.id}: {poll_error}")
                    # Fall back to cached results if polling fails
                    cached_result = self._get_cached_dt_results(sbom, mapping)
                    if cached_result:
                        cached_result.update({"action": "poll_failed_cached", "poll_error": str(poll_error)})
                        return cached_result
                    else:
                        # No cached results, return error
                        return {
                            "error": f"Failed to poll DT updates and no cached results: {poll_error}",
                            "sbom_id": str(sbom.id),
                            "provider": "dependency_track",
                            "action": "poll_failed",
                        }

        except VulnerabilityProviderError as e:
            logger.error(f"Dependency Track scan failed for SBOM {sbom.id}: {e}")
            return {"error": str(e), "sbom_id": str(sbom.id), "provider": "dependency_track"}

    def _should_upload_to_dt(self, sbom, mapping: Optional[ComponentDependencyTrackMapping]) -> bool:
        """
        Determine if we should upload SBOM to DT or just poll for updated vulnerability data.

        Logic:
        - No mapping exists → Upload (first time)
        - Mapping exists but no SBOM uploaded recently → Upload
        - Mapping exists with recent upload (< 24h) → Poll only
        - Mapping exists but old upload (> 24h) → Upload (refresh)

        Args:
            sbom: SBOM instance
            mapping: Existing ComponentDependencyTrackMapping or None

        Returns:
            True if should upload, False if should poll only
        """

        # No mapping = first time, must upload
        if not mapping:
            logger.debug(f"No DT mapping for SBOM {sbom.id}, will upload")
            return True

        # Check if this specific SBOM has been uploaded recently
        recent_threshold = timezone.now() - timedelta(hours=24)

        # Check if we have any recent scan results for this specific SBOM
        recent_dt_scan = (
            VulnerabilityScanResult.objects.filter(
                sbom=sbom, provider="dependency_track", component_mapping=mapping, created_at__gte=recent_threshold
            )
            .order_by("-created_at")
            .first()
        )

        if recent_dt_scan:
            # Check if it was an upload action (not just a poll)
            scan_metadata = recent_dt_scan.scan_metadata or {}
            last_action = scan_metadata.get("action", "unknown")

            if last_action in ["upload", "poll"]:
                logger.debug(f"SBOM {sbom.id} has recent DT scan ({last_action}), will poll for updates")
                return False

        # Check mapping's last upload timestamp
        if mapping.last_sbom_upload:
            time_since_upload = timezone.now() - mapping.last_sbom_upload

            if time_since_upload < timedelta(hours=24):
                logger.debug(f"Component mapping has recent upload ({time_since_upload}), will poll for updates")
                return False
            else:
                logger.debug(f"Component mapping upload is old ({time_since_upload}), will refresh with upload")
                return True

        # Mapping exists but no upload timestamp = upload
        logger.debug(f"Mapping exists but no upload timestamp for SBOM {sbom.id}, will upload")
        return True

    def get_team_vulnerability_provider(self, team) -> str:
        """
        Get the vulnerability scanning provider for a team.

        Args:
            team: Team instance

        Returns:
            Provider name ('osv' or 'dependency_track')
        """
        try:
            settings_obj = TeamVulnerabilitySettings.objects.get(team=team)
            return settings_obj.vulnerability_provider
        except TeamVulnerabilitySettings.DoesNotExist:
            # Default to OSV if no settings exist
            return "osv"

    def select_dependency_track_server(self, team) -> DependencyTrackServer:
        """
        Select a Dependency Track server for a team.

        - Enterprise teams: Can use custom server OR shared pool
        - Business teams: Can only use shared pool
        - Uses round-robin selection from available servers in the pool

        Args:
            team: Team instance

        Returns:
            DependencyTrackServer instance

        Raises:
            VulnerabilityProviderError: If no suitable server is available
        """
        # Check for custom DT server (Enterprise only)
        if team.billing_plan == "enterprise":
            try:
                team_settings = TeamVulnerabilitySettings.objects.get(team=team)
                if team_settings.custom_dt_server and team_settings.custom_dt_server.is_available_for_scan:
                    return team_settings.custom_dt_server
            except TeamVulnerabilitySettings.DoesNotExist:
                pass

        # Use server pool with round-robin selection
        available_servers = (
            DependencyTrackServer.objects.filter(is_active=True, health_status__in=["healthy", "degraded"])
            .exclude(current_scan_count__gte=models.F("max_concurrent_scans"))
            .order_by("priority", "current_scan_count")
        )

        if not available_servers.exists():
            raise VulnerabilityProviderError("No available Dependency Track servers")

        # Select server with lowest load in highest priority group
        selected_server = available_servers.first()

        # Increment scan count atomically
        DependencyTrackServer.objects.filter(id=selected_server.id).update(
            current_scan_count=models.F("current_scan_count") + 1
        )

        return selected_server

    def get_or_create_component_mapping(
        self, component, dt_server: DependencyTrackServer
    ) -> ComponentDependencyTrackMapping:
        """
        Get or create a mapping between sbomify component and DT project.

        Note: This method now returns None for new mappings since project creation
        happens during the first SBOM upload. The mapping is created after successful
        BOM-based project creation.

        Args:
            component: Component instance
            dt_server: DependencyTrackServer instance

        Returns:
            ComponentDependencyTrackMapping instance if existing, None if needs to be created
        """
        # Check for existing mapping
        try:
            mapping = ComponentDependencyTrackMapping.objects.get(component=component, dt_server=dt_server)
            return mapping
        except ComponentDependencyTrackMapping.DoesNotExist:
            # Return None to indicate that project creation will happen via BOM upload
            # The mapping will be created after successful BOM-based project creation
            logger.info(
                f"No existing DT project mapping for component {component.id} on server {dt_server.name}. "
                f"Project will be created via BOM upload."
            )
            return None

    def create_component_mapping_after_bom_upload(
        self, component, dt_server: DependencyTrackServer, project_name: str, project_version: str
    ) -> Optional[ComponentDependencyTrackMapping]:
        """
        Create a component mapping after successful BOM-based project creation.

        Args:
            component: Component instance
            dt_server: DependencyTrackServer instance
            project_name: Name of the created project
            project_version: Version of the created project

        Returns:
            ComponentDependencyTrackMapping instance if project found, None otherwise
        """
        try:
            # Create DT client
            client = DependencyTrackClient(dt_server.url, dt_server.api_key)

            # Find the created project
            project_data = client.find_project_by_name_version(project_name, project_version)

            if not project_data:
                logger.error(
                    f"Could not find DT project {project_name} v{project_version} after BOM upload "
                    f"for component {component.id}"
                )
                return None

            # Create mapping record
            with transaction.atomic():
                mapping = ComponentDependencyTrackMapping.objects.create(
                    component=component,
                    dt_server=dt_server,
                    dt_project_uuid=project_data["uuid"],
                    dt_project_name=project_data["name"],
                )

                logger.info(
                    f"Created DT project mapping {project_data['uuid']} for component {component.id} "
                    f"on server {dt_server.name} after BOM upload"
                )

                return mapping

        except Exception as e:
            logger.error(f"Failed to create component mapping after BOM upload for component {component.id}: {e}")
            return None

    def upload_sbom_to_dependency_track(
        self, sbom, sbom_data: bytes, mapping: Optional[ComponentDependencyTrackMapping]
    ) -> Dict[str, Any]:
        """
        Upload an SBOM to Dependency Track.

        If mapping is None, creates a new project via BOM upload.
        If mapping exists, uploads to existing project.

        Args:
            sbom: SBOM instance
            sbom_data: SBOM file content as bytes
            mapping: ComponentDependencyTrackMapping instance or None for new projects

        Returns:
            Upload result and mapping info
        """
        component = sbom.component

        try:
            if mapping:
                # Upload to existing project
                client = DependencyTrackClient(mapping.dt_server.url, mapping.dt_server.api_key)

                upload_result = client.upload_sbom(
                    project_uuid=str(mapping.dt_project_uuid), sbom_data=sbom_data, auto_create=True
                )

                # Update mapping timestamp
                mapping.last_sbom_upload = timezone.now()
                mapping.save(update_fields=["last_sbom_upload"])

                logger.info(f"Uploaded SBOM {sbom.id} to existing DT project {mapping.dt_project_uuid}")

                return {"upload_result": upload_result, "mapping": mapping, "created_project": False}

            else:
                # Create new project via BOM upload
                dt_server = self.select_dependency_track_server(component.team)
                client = DependencyTrackClient(dt_server.url, dt_server.api_key)

                # Generate project name and version with environment prefix
                env_prefix = self._get_environment_prefix()
                project_name = f"{env_prefix}-sbomify-{component.id}"
                project_version = "1.0.0"

                # Upload SBOM with project creation
                upload_result = client.upload_sbom_with_project_creation(
                    project_name=project_name, project_version=project_version, sbom_data=sbom_data, auto_create=True
                )

                logger.info(f"Uploaded SBOM {sbom.id} with DT project creation: {project_name} v{project_version}")

                # Create mapping after successful upload
                new_mapping = self.create_component_mapping_after_bom_upload(
                    component=component, dt_server=dt_server, project_name=project_name, project_version=project_version
                )

                if new_mapping:
                    new_mapping.last_sbom_upload = timezone.now()
                    new_mapping.save(update_fields=["last_sbom_upload"])

                return {
                    "upload_result": upload_result,
                    "mapping": new_mapping,
                    "created_project": True,
                    "project_name": project_name,
                    "project_version": project_version,
                }

        except DependencyTrackAPIError as e:
            logger.error(f"Failed to upload SBOM {sbom.id} to DT: {e}")
            raise VulnerabilityProviderError(f"SBOM upload failed: {e}")

        finally:
            # Always decrement scan count for the appropriate server
            if mapping:
                server_id = mapping.dt_server.id
            else:
                # Get server again if we were creating new project
                try:
                    dt_server = self.select_dependency_track_server(component.team)
                    server_id = dt_server.id
                except Exception as e:
                    logger.warning(f"Failed to get DT server for cleanup: {e}")
                    server_id = None

            if server_id:
                DependencyTrackServer.objects.filter(id=server_id).update(
                    current_scan_count=models.F("current_scan_count") - 1
                )

    def get_dependency_track_results(
        self, sbom, mapping: ComponentDependencyTrackMapping, force_refresh: bool = False
    ) -> Dict[str, Any]:
        """
        Get vulnerability scan results from Dependency Track.

        Args:
            sbom: SBOM instance
            mapping: ComponentDependencyTrackMapping instance
            force_refresh: Force refresh from DT API

        Returns:
            Vulnerability scan results
        """
        # Check for recent results first (avoid duplicate scans)
        if not force_refresh:
            cached_result = self._get_cached_dt_results(sbom, mapping)
            if cached_result:
                return cached_result

        try:
            client = DependencyTrackClient(mapping.dt_server.url, mapping.dt_server.api_key)

            # Get project metrics
            metrics = client.get_project_metrics(str(mapping.dt_project_uuid))

            # Get vulnerabilities
            vulnerabilities = client.get_project_vulnerabilities(str(mapping.dt_project_uuid))

            # Process and structure results
            results = self._process_dt_results(metrics, vulnerabilities)

            # Store results in PostgreSQL
            self._cache_dt_results(sbom, mapping, results)

            # Update mapping sync timestamp
            mapping.last_metrics_sync = timezone.now()
            mapping.save(update_fields=["last_metrics_sync"])

            return results

        except DependencyTrackAPIError as e:
            logger.error(f"Failed to get DT results for SBOM {sbom.id}: {e}")
            raise VulnerabilityProviderError(f"Failed to get DT results: {e}")

    def _get_cached_osv_results(self, sbom) -> Optional[Dict[str, Any]]:
        """Get recent OSV results (within last 24 hours)."""
        try:
            from datetime import timedelta

            # Check for recent results within 24 hours
            recent_threshold = timezone.now() - timedelta(hours=24)
            result = (
                VulnerabilityScanResult.objects.filter(sbom=sbom, provider="osv", created_at__gte=recent_threshold)
                .order_by("-created_at")
                .first()
            )

            if result:
                return {
                    "vulnerability_count": result.vulnerability_count,
                    "findings": result.findings,
                    "provider": "osv",
                    "cached": True,
                    "scan_date": result.created_at.isoformat(),
                    "sbom_id": str(sbom.id),
                }
        except Exception as e:
            logger.warning(f"Failed to get recent OSV results: {e}")

        return None

    def _cache_osv_results(self, sbom, results: Dict[str, Any]) -> None:
        """Store OSV results in standardized format."""
        try:
            # Standardize the results using our unified processor
            standardized_results = StandardizedVulnerabilityData.normalize_scan_results(results, "osv", "upload")

            # Store results (always create new record for historical tracking)
            VulnerabilityScanResult.objects.create(
                sbom=sbom,
                provider="osv",
                scan_trigger="upload",  # Default trigger
                vulnerability_count=standardized_results["vulnerability_count"],
                findings=standardized_results["findings"],
                scan_metadata=standardized_results["scan_metadata"],
                total_vulnerabilities=standardized_results["vulnerability_count"].get("total", 0),
                critical_vulnerabilities=standardized_results["vulnerability_count"].get("critical", 0),
                high_vulnerabilities=standardized_results["vulnerability_count"].get("high", 0),
                medium_vulnerabilities=standardized_results["vulnerability_count"].get("medium", 0),
                low_vulnerabilities=standardized_results["vulnerability_count"].get("low", 0),
                component_mapping=None,  # OSV doesn't use mappings
            )

        except Exception as e:
            logger.warning(f"Failed to store OSV results: {e}")

    def _get_cached_dt_results(self, sbom, mapping: ComponentDependencyTrackMapping = None) -> Optional[Dict[str, Any]]:
        """Get recent Dependency Track results (within last 24 hours)."""
        try:
            from datetime import timedelta

            # Check for recent results within 24 hours
            recent_threshold = timezone.now() - timedelta(hours=24)
            query_filter = {"sbom": sbom, "provider": "dependency_track", "created_at__gte": recent_threshold}

            if mapping:
                query_filter["component_mapping"] = mapping

            result = VulnerabilityScanResult.objects.filter(**query_filter).order_by("-created_at").first()

            if result:
                return {
                    "vulnerability_count": result.vulnerability_count,
                    "findings": result.findings,
                    "scan_metadata": result.scan_metadata,
                    "provider": "dependency_track",
                    "cached": True,
                    "scan_date": result.created_at.isoformat(),
                    "sbom_id": str(sbom.id),
                }
        except Exception as e:
            logger.warning(f"Failed to get recent DT results: {e}")

        return None

    def check_dependency_track_server_health(self, server: DependencyTrackServer) -> Dict[str, Any]:
        """
        Check the health of a Dependency Track server and update its status.

        Args:
            server: DependencyTrackServer instance to check

        Returns:
            Health check results
        """
        logger.info(f"Starting health check for DT server: {server.name}")

        try:
            client = DependencyTrackClient(server.url, server.api_key)

            # Perform health check
            health_result = client.health_check()

            # Update server status
            server.health_status = "healthy"
            server.last_health_check = timezone.now()
            server.save(update_fields=["health_status", "last_health_check"])

            logger.info(f"Health check successful for DT server: {server.name}")

            return {
                "server_id": str(server.id),
                "server_name": server.name,
                "status": "healthy",
                "application": health_result.get("application", "Unknown"),
                "version": health_result.get("version", "Unknown"),
                "timestamp": health_result.get("timestamp"),
                "message": "Health check successful",
            }

        except Exception as e:
            # Update server status to unhealthy
            server.health_status = "unhealthy"
            server.last_health_check = timezone.now()
            server.save(update_fields=["health_status", "last_health_check"])

            logger.error(f"Health check failed for DT server: {server.name}: {e}")

            return {
                "server_id": str(server.id),
                "server_name": server.name,
                "status": "unhealthy",
                "error": str(e),
                "message": "Health check failed",
            }

    def check_all_dependency_track_servers_health(self) -> List[Dict[str, Any]]:
        """
        Check the health of all active Dependency Track servers.

        Returns:
            List of health check results for all servers
        """
        servers = DependencyTrackServer.objects.filter(is_active=True)
        results = []

        for server in servers:
            result = self.check_dependency_track_server_health(server)
            results.append(result)

        logger.info(f"Completed health checks for {len(results)} DT servers")
        return results

    def _cache_dt_results(self, sbom, mapping: ComponentDependencyTrackMapping, results: Dict[str, Any]) -> None:
        """Store Dependency Track results in standardized format."""
        try:
            # Standardize the results using our unified processor
            standardized_results = StandardizedVulnerabilityData.normalize_scan_results(
                results, "dependency_track", "upload"
            )

            # Store results (always create new record for historical tracking)
            VulnerabilityScanResult.objects.create(
                sbom=sbom,
                component_mapping=mapping,
                provider="dependency_track",
                scan_trigger="upload",  # Default trigger
                vulnerability_count=standardized_results["vulnerability_count"],
                findings=standardized_results["findings"],
                scan_metadata=standardized_results["scan_metadata"],
                total_vulnerabilities=standardized_results["vulnerability_count"].get("total", 0),
                critical_vulnerabilities=standardized_results["vulnerability_count"].get("critical", 0),
                high_vulnerabilities=standardized_results["vulnerability_count"].get("high", 0),
                medium_vulnerabilities=standardized_results["vulnerability_count"].get("medium", 0),
                low_vulnerabilities=standardized_results["vulnerability_count"].get("low", 0),
            )

        except Exception as e:
            logger.warning(f"Failed to store DT results: {e}")

    def _process_dt_results(self, metrics: Dict[str, Any], vulnerabilities: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Process raw Dependency Track results into standardized format.

        Args:
            metrics: Project metrics from DT
            vulnerabilities: Vulnerability list from DT

        Returns:
            Processed results in standard format
        """
        # Count unique vulnerabilities by severity (not component-vulnerability pairs)
        severity_counts = {}
        unique_vulns = set()

        # First pass: count unique vulnerabilities by severity
        for vuln in vulnerabilities:
            vuln_id = vuln.get("vulnId")
            severity = vuln.get("severity", "UNKNOWN").lower()

            if vuln_id not in unique_vulns:
                unique_vulns.add(vuln_id)
                if severity in ["critical", "high", "medium", "low", "info"]:
                    severity_counts[severity] = severity_counts.get(severity, 0) + 1

        vuln_counts = {
            "critical": severity_counts.get("critical", 0),
            "high": severity_counts.get("high", 0),
            "medium": severity_counts.get("medium", 0),
            "low": severity_counts.get("low", 0),
            "info": severity_counts.get("info", 0),
            "total": len(unique_vulns),
        }

        # Process vulnerabilities (may create multiple entries for multi-component vulns)
        processed_vulns = []
        for vuln in vulnerabilities:
            severity = vuln.get("severity", "UNKNOWN").lower()

            # Extract components (DT returns array of affected components)
            components = vuln.get("components", [])

            # Create vulnerability entry for each affected component
            if components:
                for component in components:
                    # Extract ecosystem from purl
                    purl = component.get("purl", "")
                    ecosystem = "Unknown"
                    if purl.startswith("pkg:"):
                        try:
                            ecosystem = purl.split(":")[1].split("/")[0]
                        except (IndexError, AttributeError):
                            ecosystem = "Unknown"

                    processed_vulns.append(
                        {
                            "id": vuln.get("vulnId"),
                            "source": vuln.get("source", "Dependency Track"),
                            "severity": severity,
                            "title": vuln.get("description", "")[:100] + "..."
                            if vuln.get("description")
                            else "No description",  # Use description as title, truncated
                            "summary": vuln.get("description", ""),  # Full description as summary
                            "description": vuln.get("description", ""),
                            "recommendation": vuln.get("recommendation", ""),
                            "references": vuln.get("references", [])
                            if isinstance(vuln.get("references", []), list)
                            else [],
                            "cvss_score": vuln.get("cvssV3BaseScore") or vuln.get("cvssV2BaseScore"),
                            "aliases": vuln.get("aliases", []),
                            "component": {
                                "name": component.get("name", "Unknown"),
                                "version": component.get("version", "Unknown"),
                                "ecosystem": ecosystem,  # This will be pypi, npm, etc.
                                "purl": component.get("purl", ""),
                            },
                        }
                    )
            else:
                # No components found, create entry with unknown component
                processed_vulns.append(
                    {
                        "id": vuln.get("vulnId"),
                        "source": vuln.get("source", "Dependency Track"),
                        "severity": severity,
                        "title": vuln.get("description", "")[:100] + "..."
                        if vuln.get("description")
                        else "No description",
                        "summary": vuln.get("description", ""),
                        "description": vuln.get("description", ""),
                        "recommendation": vuln.get("recommendation", ""),
                        "references": vuln.get("references", []),
                        "cvss_score": vuln.get("cvssV3BaseScore") or vuln.get("cvssV2BaseScore"),
                        "aliases": vuln.get("aliases", []),
                        "component": {
                            "name": "Unknown Package",
                            "version": "Unknown Version",
                            "ecosystem": "Unknown",
                            "purl": "",
                        },
                    }
                )

        return {
            "vulnerability_count": vuln_counts,
            "findings": processed_vulns,
            "metrics": metrics,
            "provider": "dependency_track",
            "scan_timestamp": timezone.now().isoformat(),
        }


# Helper functions for weekly vulnerability scanning


def get_weekly_scan_targets(days_back: int, team_key: str = None, max_releases: int = None):
    """Get list of releases to scan for weekly vulnerability scanning."""
    from django.utils import timezone

    from core.models import Release

    queryset = Release.objects.select_related(
        "product", "product__component", "product__component__team"
    ).prefetch_related("sboms")

    # Filter by team if specified
    if team_key:
        queryset = queryset.filter(product__component__team__key=team_key)

    # Filter by creation date
    if days_back:
        cutoff_date = timezone.now() - timedelta(days=days_back)
        queryset = queryset.filter(created_at__gte=cutoff_date)

    # Only get releases that have SBOMs
    queryset = queryset.filter(sboms__isnull=False).distinct()

    # Apply max limit if specified
    if max_releases:
        queryset = queryset[:max_releases]

    releases = list(queryset)
    logger.info(f"Found {len(releases)} releases with SBOMs for weekly scanning")
    return releases


def perform_weekly_scans(releases, force_rescan: bool = False):
    """Perform vulnerability scans on all releases for weekly scanning."""

    results = {
        "total_releases": len(releases),
        "total_sboms": 0,
        "successful_scans": 0,
        "failed_scans": 0,
        "skipped_scans": 0,
        "errors": [],
        "provider_stats": {},
    }

    service = VulnerabilityScanningService()

    for i, release in enumerate(releases, 1):
        logger.info(f"[WEEKLY_SCAN] [{i}/{len(releases)}] Scanning {release.product.name} v{release.name}...")

        # OSV vulnerability scanning is available for ALL teams
        # The VulnerabilityScanningService will handle provider selection

        # Scan all SBOMs for this release
        for sbom in release.sboms.all():
            results["total_sboms"] += 1

            try:
                # Check if recent scan exists (unless force rescan)
                if not force_rescan and _has_recent_weekly_scan(sbom):
                    logger.info(f"[WEEKLY_SCAN] Skipping {sbom.name} - recent scan exists")
                    results["skipped_scans"] += 1
                    continue

                # Perform scan
                scan_result = _scan_sbom_for_weekly(sbom, service)

                if scan_result and scan_result.get("status") != "error":
                    results["successful_scans"] += 1
                    provider = scan_result.get("provider", "unknown")
                    results["provider_stats"][provider] = results["provider_stats"].get(provider, 0) + 1
                    logger.info(f"[WEEKLY_SCAN] Successfully scanned {sbom.name} with {provider}")
                else:
                    results["failed_scans"] += 1
                    error_msg = scan_result.get("error", "Unknown error") if scan_result else "Scan returned None"
                    results["errors"].append(f"Failed to scan {sbom.name}: {error_msg}")
                    logger.error(f"[WEEKLY_SCAN] Failed to scan {sbom.name}: {error_msg}")

            except Exception as e:
                results["failed_scans"] += 1
                error_msg = f"Failed to scan {sbom.name}: {e}"
                results["errors"].append(error_msg)
                logger.exception(f"[WEEKLY_SCAN] {error_msg}")

    return results


def _has_recent_weekly_scan(sbom) -> bool:
    """Check if SBOM has been scanned recently (within 24 hours)."""
    from django.utils import timezone

    cutoff = timezone.now() - timedelta(hours=24)
    return VulnerabilityScanResult.objects.filter(sbom=sbom, created_at__gte=cutoff).exists()


def _scan_sbom_for_weekly(sbom, service):
    """Scan a single SBOM for vulnerabilities in weekly context."""
    try:
        # Get SBOM data from S3
        from core.object_store import S3Client

        s3_client = S3Client(bucket_type="SBOMS")
        sbom_data = s3_client.get_sbom_data(sbom.sbom_filename)
        if not sbom_data:
            logger.error(f"Failed to download SBOM {sbom.sbom_filename} from S3")
            return None

        # Perform scan with weekly trigger
        scan_result = service.scan_sbom_for_vulnerabilities(sbom=sbom, sbom_data=sbom_data, scan_trigger="weekly")

        return scan_result

    except Exception as e:
        logger.error(f"Failed to scan SBOM {sbom.id} in weekly context: {e}")
        return None
